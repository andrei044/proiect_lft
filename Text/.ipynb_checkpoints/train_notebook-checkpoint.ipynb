{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff56559-e371-49f2-9e96-8278618a9e66",
   "metadata": {},
   "source": [
    "# Load the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c4a1b80-42b4-44ce-b73d-2bba5efeed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d076a7d-1146-4d04-8769-811898b8406f",
   "metadata": {},
   "source": [
    "# Test model for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3efd585b-9015-4bd3-8668-0d89b2e3060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a mystical forest, there lived a [IMAGE: magical creature with wings]. One day, the creature found a [IMAGE: mysterious object] that changed everything. This had just been there for a long time, he explained. But now he had discovered that something had changed it as well. The creature wanted to take the message...And now the message he sent to his son was coming...but even through its darkness, the message remained as it was.\n",
      "\n",
      "\n",
      "In a distant world, a girl named Kondu was in the forest, and she had seen something strange. This was what she thought. When she came across this magical creature, she looked up and it turned from Kondu to its original shape, but it was no more. The creature did not like her. It seemed to have moved on to a new, faraway world that it had always wanted and had only been searching for for the very last. It appeared in its new position, and Kondu was terrified, and the creature took what became of her. In a way, it looked like the creature had become the person she had known, had disappeared... but not\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "prompt = \"Once upon a time in a mystical forest, there lived a [IMAGE: magical creature with wings]. One day, the creature found a [IMAGE: mysterious object] that changed everything.\"\n",
    "generated_text = pipe(prompt, max_new_tokens=200, do_sample=True)\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972282b8-74df-4eb4-8735-e0f88fbd46dd",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "be60ddfd-2a09-4780-ba42-b07909c5f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "#dataset = load_dataset(\"Dwaraka/Training_Dataset_of_Project_Gutebberg_Gothic_Fiction\")\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "# already processed dataset(added the image tags)\n",
    "dataset=DatasetDict.load_from_disk(\"./modified_datasets1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff43e8d8-3353-494d-97cb-9f4f7d1096c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 92100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a8d7e7-2a15-41f1-ab7a-6f7c7bb8314a",
   "metadata": {},
   "source": [
    "## Clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "670ac89d-a6a7-4810-af1a-65f0a33512f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# check for these strings\n",
    "strings_to_find = [\n",
    "    \"Act.\", \"*\", \"CHAPTER\"\n",
    "]\n",
    "\n",
    "#function to filter strings\n",
    "def filter_strings(strings):\n",
    "    filtered_strings = []\n",
    "    for string in strings:\n",
    "        # Check for empty string\n",
    "        if not string.strip():\n",
    "            continue\n",
    "        # Check for strings_to_find in the string\n",
    "        # for s in strings_to_find:\n",
    "        #     if s in string:\n",
    "        #         continue\n",
    "        if \"Act.\" in string or \"*\" in string or \"CHAPTER\" in string:\n",
    "            continue\n",
    "        # Check if the string contains only uppercase letters\n",
    "        if string.isupper():\n",
    "            continue\n",
    "        # Check for any other symbol (non-alphanumeric characters)\n",
    "        #if re.search(r'[^a-zA-Z0-9\\s]', string):\n",
    "            #continue\n",
    "        # If all checks are passed, add the string to the filtered list\n",
    "        filtered_strings.append(string)\n",
    "    return filtered_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4df8fcb4-d430-4e02-b4ee-4efaafd174be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Get the 'train' split\n",
    "text = dataset['train']['text']\n",
    "\n",
    "# Convert to a list of dictionaries, drop the first 62 elements, and convert back\n",
    "remaining_data = text[62:]\n",
    "\n",
    "remaining_data = filter_strings(remaining_data)\n",
    "\n",
    "# Create a new Dataset from the remaining data\n",
    "remaining_dataset = DatasetDict({'train':Dataset.from_dict({'text': remaining_data}) })\n",
    "\n",
    "# Replace the 'train' split with the new dataset in the modified_datasets\n",
    "dataset = remaining_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1cecd658-ccfe-438c-b5c2-a2637d0dde58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 79252\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae579d-18a4-4c16-9d64-60a32fa274b4",
   "metadata": {},
   "source": [
    "# Generate [IMAGE:{desc}] tags in the dataset (Optional if using the unprocessed dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae57456-dd84-42c9-93c0-759f473e638b",
   "metadata": {},
   "source": [
    "## Initialize the pipeline for description generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f7b4c99-3fbc-412f-af81-830d0c38e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text generation pipeline for description generation\n",
    "description_generator = pipeline(\"text-generation\", model=model_name, tokenizer=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb046bb-dab8-43bc-8c43-809f4c16cc97",
   "metadata": {},
   "source": [
    "### Function to generate image descriptions based on text context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "84c6e3fe-711e-476b-acd1-b3d45cdb2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(text):\n",
    "    prompt = \"Describe the following scene in detail: \" + text\n",
    "    generated = description_generator(prompt, max_new_tokens=10, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    description = generated[0]['generated_text'].replace(prompt, \"\").strip()\n",
    "    return description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d4c1e-edf2-4852-b508-36df0b1bffc0",
   "metadata": {},
   "source": [
    "### Function to randomly insert image tags with descriptions into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2ea2a31d-916e-42dc-860a-484a9d4d760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def insert_image_tags(text, interval_range=(1, 100)):\n",
    "    words = text.split()\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        interval = random.randint(*interval_range)\n",
    "        i += interval\n",
    "        if i < len(words):\n",
    "            context = \" \".join(words[max(0, i-50):i+50])  # Provide some context for generating description\n",
    "            description = generate_description(context)\n",
    "            words.insert(i, f\"[IMAGE:{description}]\")\n",
    "            i += 1  # Move past the inserted tag\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11490ec-1ed0-4e39-9b45-6093ed78de52",
   "metadata": {},
   "source": [
    "### Apply the function to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3a4583d1-c7d9-4ea6-93d1-4195ed75f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_image_tags(examples):\n",
    "    examples['text'] = [insert_image_tags(text) for text in examples['text']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "22d776fb-1299-47f1-a02c-811d95be4cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set pad token to the one from tokenizer\n",
    "model.generation_config.pad_token_ids = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "66758585-3e94-4489-820f-2aca0940bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the dataset to include image tags\n",
    "#modified_datasets = dataset.map(add_image_tags, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ab0f3e7-382c-4eea-8e3e-2a556d6da8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified_datasets.save_to_disk(\"./modified_datasets1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3eee6141-3165-47ec-9b8c-3e75ebc41332",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_datasets=dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c301884a-7a1e-43a8-857f-eb4a8f319793",
   "metadata": {},
   "source": [
    "# Tokenize the dataset (split every sentence into tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "65bce01b-1399-4251-9559-4d64ceb806d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d303c71c53e4fc8b143a2332fbe5726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79252 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = modified_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "81977947-28de-4cdc-a883-af99a747ac00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 79252\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "548d88bd-4e6d-47ee-8a96-0fc7799400e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d5d1d10a824e4789b6fd96feec2248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79252 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    # We drop the last block if it's smaller than block_size\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    \n",
    "    # Split by chunks of block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b9b12ee2-cf0b-495a-8eee-bebb05457816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 10252\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7972e522-581c-4b34-a0bc-ea6607b4a579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='257' max='257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [257/257 01:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=257, training_loss=4.19561791327213, metrics={'train_runtime': 60.4588, 'train_samples_per_second': 8.479, 'train_steps_per_second': 4.251, 'total_flos': 33576026112000.0, 'train_loss': 4.19561791327213, 'epoch': 0.050136558720249705})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# Use DataCollatorForLanguageModeling to dynamically pad the inputs received by the model\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=0.05,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets['train'],\n",
    "    # eval_dataset=lm_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b3c50-3c9b-4179-a2bb-e076690970cb",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1f5dfef2-c8c4-418c-baec-842bda62b349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2-finetuned-fiction\\\\tokenizer_config.json',\n",
       " './gpt2-finetuned-fiction\\\\special_tokens_map.json',\n",
       " './gpt2-finetuned-fiction\\\\vocab.json',\n",
       " './gpt2-finetuned-fiction\\\\merges.txt',\n",
       " './gpt2-finetuned-fiction\\\\added_tokens.json')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./gpt2-finetuned-fiction\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetuned-fiction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaf6f16-77a7-47c1-a6bf-6c6e14913595",
   "metadata": {},
   "source": [
    "# Test the new saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "de8d9cf7-b549-4a9b-87ac-f860f48e0fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-finetuned-fiction\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned-fiction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "db1d8175-b6c5-4581-b70c-0f65e8a565a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a mystical forest, there lived a [IMAGE: magical creature with wings]. One day, the creature found a [IMAGE: mysterious object] that changed everything. The strange creature, awakened by [IMAGE: magic of the past, present, and future, but it could not] its power, and proceeded to do all we might, in pursuit not of revenge butof some new and greater purpose, thegreat, true, oratorical ambition. One night, as [IMAGE: was being summoned, all thoughts became wild, and, looking towards him, became] aware of the disturbance, which was going on on the forest and in its surroundings, the thing drew nearer. A shadow from its midst fell on the ground, and, being startled, it went from the shadows, and, being heard and seen by the [IMAGE:the shadow and being seen, the shadow stopped its speech. It looked] inhabitants of thatplace, [IMAGE:the shadow, as they recognized him, did some] proceeded towards [IMAGE:the shadow, again, it ceased its speech.] the forest, [IMAGE:the shadow,\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "prompt = \"Once upon a time in a mystical forest, there lived a [IMAGE: magical creature with wings]. One day, the creature found a [IMAGE: mysterious object] that changed everything.\"\n",
    "generated_text = pipe(prompt, max_new_tokens=200, do_sample=True)\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a20af2c-126f-4df9-b8f9-3f144976a028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
